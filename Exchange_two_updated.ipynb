{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                   \n",
    "from bs4 import BeautifulSoup                          \n",
    "import requests  \n",
    "import datetime\n",
    "from time import sleep\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Access-Control-Allow-Origin': '*',\n",
    "    'Access-Control-Allow-Methods': 'GET',\n",
    "    'Access-Control-Allow-Headers': 'Content-Type',\n",
    "    'Access-Control-Max-Age': '3600',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.86 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('exchange_all_listings.xlsx')\n",
    "all_links = df['links'].tolist()[:20]\n",
    "# print(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "list = {}\n",
    "counts = 0\n",
    "name = 'Exchange'\n",
    "for link in all_links:\n",
    "    counts += 1\n",
    "    print(counts)\n",
    "    rs = requests.get(link, headers= headers)\n",
    "    soup = BeautifulSoup(rs.content, 'html.parser')\n",
    "    sleep(2)\n",
    "#     print(rs.content)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "    try:   \n",
    "        try:\n",
    "            ids = soup.find('h1',attrs= {'class':'_3c7Mo _3Rk8c _1-Yji'}).get_text().split('#')[1]\n",
    "        except:\n",
    "            ids = soup.find('h1',attrs= {'class':'_3c7Mo _3Rk8c _1-Yji'}).get_text()\n",
    "    except:\n",
    "        ids = 'Not Found'\n",
    "#     print(ids)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "    \n",
    "    try:\n",
    "        domain = soup.find('a',attrs= {'class':'_2gej5 _3dg_f _3c7Mo GbPX0 _1-Yji'}).get('href')\n",
    "    except:\n",
    "        domain = soup.find('span',attrs= {'class':'_3c7Mo _1-Yji'}).get_text().strip()\n",
    "#     print(domain)  \n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "    \n",
    "    platform = 'Shopify'\n",
    "#   platform = soup.find('div',attrs= {'id':'platform'}).get_text().strip()\n",
    "#     print(platform)\n",
    "    \n",
    "#     industry = '-'\n",
    "#     print(industry)\n",
    "\n",
    "#     country = 'null'\n",
    "    \n",
    "    try: \n",
    "        category = soup.find('a',attrs= {'class':'_3vKm9 jmdHf'}).get_text().strip()\n",
    "    except:\n",
    "        category = '-'\n",
    "        pass\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "    \n",
    "    try:\n",
    "        monitization = soup.findAll('div',attrs= {'class':'_2ONvs _2vj1S _33wM-'})[2].findAll('span')[1].get_text().replace('\"',' ').strip()\n",
    "    except:\n",
    "        pass\n",
    "#     print(monitization)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "  \n",
    "    try: \n",
    "        site_age = soup.find('div',attrs= {'class':'_2ONvs _2vj1S _33wM-'}).findAll('span')[1].get_text().replace('\"',' ').strip()\n",
    "    except:\n",
    "        pass\n",
    "#     print(site_age)\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    " \n",
    "    try:\n",
    "        avg_prof_month = soup.findAll('div',attrs= {'class':'_1uCwB LcAyu'})[1].findAll('span')[1].get_text().replace('\"',' ').strip()\n",
    "    except:\n",
    "        pass\n",
    "#     print(avg_prof_month)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    " \n",
    "    try:\n",
    "        avg_rev_month = soup.findAll('div',attrs= {'class':'_1uCwB LcAyu'})[0].findAll('span')[1].get_text().replace('\"',' ').strip()\n",
    "    except:\n",
    "        pass\n",
    "#     print(avg_rev_month)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "    try:\n",
    "        avg_traffic_month = soup.findAll('li',attrs= {'class':'_2v9K-'})[1].findAll('span')[2].get_text().replace('\"',' ').strip()\n",
    "    except:\n",
    "        pass\n",
    "#     print(avg_traffic_month)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    " \n",
    "    try:\n",
    "        bid_price = soup.findAll('span',attrs = {'class':'_3sxJP'})[1].get_text().strip()\n",
    "        bid_price = f'${bid_price}'\n",
    "    except:\n",
    "        bid_price = '-'\n",
    "        pass\n",
    "#     print(bid_price)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "    try:\n",
    "        about = soup.find('section',attrs= {'id':'BusinessStory'}).get_text().strip()\n",
    "    except:\n",
    "        about = '-'\n",
    "#     print(about)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    " \n",
    "    try:\n",
    "        seller_name = soup.find('span',attrs= {'class':'_2a41F _3c7Mo _149MA _1-Yji spVWc'}).get_text()\n",
    "    except:\n",
    "        seller_name = '-'\n",
    "#     print(seller_name)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "    try: \n",
    "        time_to_run = soup.findAll('div',attrs= {'class':'_2ONvs _2vj1S _33wM-'})[1].findAll('span')[1].get_text().replace('\"',' ').strip()\n",
    "    except:\n",
    "        time_to_run='-'\n",
    "        pass\n",
    "#     print(time_to_run)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "  \n",
    "    try:\n",
    "        profit_margin = soup.findAll('span',attrs={'class':'_3Gxwk'})[2].get_text().strip()\n",
    "        \n",
    "    except:\n",
    "        profit_margin = '-'\n",
    "#     print(profit_margin)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "    \n",
    "    try:\n",
    "        avg_sale_month = soup.findAll('span',attrs={'class':'_3Gxwk'})[3].get_text().strip()\n",
    "    except:\n",
    "        profit_margin = '-'\n",
    "#     print(avg_sale_month)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "  \n",
    "    try:\n",
    "        total_revenue = soup.findAll('div',attrs={'class':'_1uCwB LcAyu'})[4].get_text()\n",
    "    except:\n",
    "        total_revenue = '-'\n",
    "#     print(total_revenue)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "      \n",
    "    try:\n",
    "        total_traffic = soup.findAll('span',attrs={'class':'_3Gxwk'})[4].get_text().strip()\n",
    "        \n",
    "    except:\n",
    "        total_traffic = '-'\n",
    "#     print(total_traffic)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "  \n",
    "    try:\n",
    "        mailing_list = soup.findAll('span',attrs={'class':'_3Gxwk'})[5].get_text().strip()\n",
    "        \n",
    "    except:\n",
    "        mailing_list = '-'\n",
    "#     print(mailing_list)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "  \n",
    "    try:\n",
    "        expences = soup.find('ul',attrs={'class':'_2UlBf'}).findAll('li')\n",
    "        expence = []\n",
    "        for e in expences:\n",
    "            a = e.find('dt').get_text()\n",
    "            b= e.find('dd').get_text()\n",
    "            expence1 = f'{a} - {b}'\n",
    "            expence.append(expence1)\n",
    "#         print(expence)\n",
    "        e_1 = expence[1]\n",
    "        e_2 = expence[2]\n",
    "        e_3 = expence[3]\n",
    "      \n",
    "    except:\n",
    "        e_1 = '-'\n",
    "        e_2 = '-'\n",
    "        e_3 = '-'\n",
    "        pass\n",
    "    sleep(1)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "    \n",
    "    jsons = soup.findAll('script',attrs={'type':'application/json'})[1].get_text()[4:-3]\n",
    "    m = json.loads(jsons)\n",
    "    with open('data_exchange.json', 'w') as outfile:\n",
    "        json.dump(m, outfile)\n",
    "    sleep(1)\n",
    "    with open('data_exchange.json','r') as file:\n",
    "        datas = json.load(file)\n",
    "#     print(datas)\n",
    "#     sleep(1)\n",
    "    try:\n",
    "        date_with_rev = []\n",
    "        date_rev = datas.get('shop').get('monthByMonthRevenue')\n",
    "        for i in date_rev:\n",
    "            date = i.get('date')\n",
    "            rev = i.get('revenue')\n",
    "            date_revenue = f'{date} : ${rev}'\n",
    "            date_with_rev.append(date_revenue)\n",
    "        revenues = f'{date_with_rev[-1]}\\n{date_with_rev[-2]}\\n{date_with_rev[-3]}\\n{date_with_rev[-4]}\\n{date_with_rev[-5]}\\n{date_with_rev[-6]}'\n",
    "    except:\n",
    "        revenues = '-'\n",
    "#     print(revenues)\n",
    "    try:\n",
    "        date_with_traf = []\n",
    "        date_traf = datas.get('shop').get('monthByMonthTraffic')\n",
    "        for i in date_traf:\n",
    "            date = i.get('date')\n",
    "            traf = i.get('traffic')\n",
    "            date_traffic = f'{date} : {traf}'\n",
    "            date_with_traf.append(date_traffic)\n",
    "        traffics = f'{date_with_traf[-1]}\\n{date_with_traf[-2]}\\n{date_with_traf[-3]}\\n{date_with_traf[-4]}\\n{date_with_traf[-5]}\\n{date_with_traf[-6]}'\n",
    "    except:\n",
    "        traffics = '-'\n",
    "#     print(traffics)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------    \n",
    "       \n",
    "    sleep(1)\n",
    "        \n",
    "    list = {\n",
    "        'Website': name,\n",
    "        'Id': ids,\n",
    "        'Url': link,\n",
    "        'Domain': domain,\n",
    "        'Platform': platform,\n",
    "#         'Industry':industry,\n",
    "#         'Country':country,\n",
    "        'Category':category,\n",
    "        'Monetization':monitization,\n",
    "        'Site Age': site_age,\n",
    "        'profit_margin': profit_margin,\n",
    "        'Avg sale p/m': avg_sale_month,\n",
    "        'Total Revenue': total_revenue,\n",
    "        'Total Traffic': total_traffic,\n",
    "        'Total Mailing List': mailing_list,\n",
    "        'Time to run the bussiness': time_to_run,\n",
    "        'Avg net profit per Month $': avg_prof_month,\n",
    "        'Avg net revenue per Month $': avg_rev_month,\n",
    "        'Avg monthly traffic unique': avg_traffic_month,\n",
    "        'Revenue Months': revenues,\n",
    "        'Traffic Months': traffics,\n",
    "        'Bid Price':bid_price,\n",
    "        'About':about,\n",
    "        'Expences 1': e_1,\n",
    "        'Expences 2': e_2,\n",
    "        'Expences 3': e_3,\n",
    "        'Seller Name': seller_name\n",
    "    }\n",
    "    data.append(list)\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Final_exchange.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
